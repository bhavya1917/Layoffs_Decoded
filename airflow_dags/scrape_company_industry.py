import json
import pendulum
import pandas as pd
import numpy as np
import uuid
import datetime
import time
import requests

from airflow.decorators import dag, task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator
from airflow.operators.bash import BashOperator
from airflow.models import Variable

from data_preparation.scraping import scrape_company_industry

@dag(
    schedule_interval=None,
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    catchup=False,
    tags=["scraping"],
    concurrency=10,
    max_active_runs=1,
    params = {
        "companySymbolCsv": "warn_jaccard_0.5_cleaned.csv"
    }
)
def scrape_industry():
    s3_bucket = Variable.get("S3_BUCKET", default_var="layoffs-decoded-master")
    # symbols = ['AAPL', 'GOOGL', 'MSFT']

    @task(
        retries=2,
        execution_timeout=datetime.timedelta(minutes=3),
        retry_delay=datetime.timedelta(minutes=2),
    )
    def extract_company_industry(symbols, output_dir):
        """
        Extract company size data for a given set of symbols and time period
        """
        api_key = Variable.get("FMP_API_KEY", default_var="")
        output = scrape_company_industry.extract_company_data(symbols, output_dir, api_key)
        return output

    @task(
        retries=2,
        execution_timeout=datetime.timedelta(minutes=1),
        retry_delay=datetime.timedelta(minutes=1),
    )
    def retrieve_company_symbols(output_dir, **ctx):
        csv_path = ctx['params']['companySymbolCsv']
        s3_hook = S3Hook()
        s3_hook.download_file(
            key=csv_path,
            bucket_name=s3_bucket,
            preserve_file_name=True,
            use_autogenerated_subdir=False,
            local_path=output_dir
        )
        symbol_df = pd.read_csv(f"{output_dir}/{csv_path}")
        symbols = symbol_df['Symbol'].tolist()
        return symbols
        # symbol_slices = [x.tolist() for x in np.array_split(symbols, int(len(symbols)/10))]
        # return list(symbol_slices)

    @task(
        retries=2,
        execution_timeout=datetime.timedelta(minutes=3),
        retry_delay=datetime.timedelta(minutes=1),
    )
    def upload_csv_s3(local_file_path, s3_bucket):
        """
        Upload output CSV to S3 bucket
        """
        s3_hook = S3Hook()
        s3_hook.load_file(local_file_path, f"company_industry_data.csv", s3_bucket, replace=True)

    create_tmp_dir = BashOperator(
        task_id="create_tmp_dir",
        bash_command="mktemp -d 2>/dev/null"
    )
    company_symbol_list = retrieve_company_symbols(output_dir=create_tmp_dir.output)
    company_size_csv_path = extract_company_industry(company_symbol_list, output_dir=create_tmp_dir.output)
    upload_res = upload_csv_s3(company_size_csv_path, s3_bucket=s3_bucket)
    remove_tmp_dir = BashOperator(
        task_id="remove_tmp_dir",
        bash_command="rm -rf {{ ti.xcom_pull(task_ids='create_tmp_dir') }}"
    )
    upload_res >> remove_tmp_dir

scrape_industry()
