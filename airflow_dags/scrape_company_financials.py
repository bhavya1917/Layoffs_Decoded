import json

import pendulum
import pandas as pd
import numpy as np
import uuid
import datetime
import time

from airflow.decorators import dag, task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.transfers.local_to_s3 import LocalFilesystemToS3Operator
from airflow.operators.bash import BashOperator
from airflow.models import Variable

from data_preparation.scraping import scrape_financials_fmp

@dag(
    schedule_interval='@daily',
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    catchup=False,
    tags=["scraping"],
    concurrency=10,
    max_active_runs=1,
    params={
        "companySymbolCsv": "warn_jaccard_0.5_cleaned.csv",
        "symbolColumn": "Symbol"
    }
)
def scrape_company_financials():
    s3_bucket = Variable.get("S3_BUCKET", default_var="layoffs-decoded-master")
    """
    Scrape company financial data from FMP at https://site.financialmodelingprep.com/developer/docs
    """
    @task(
        retries=2,
        execution_timeout=datetime.timedelta(minutes=3),
        retry_delay=datetime.timedelta(minutes=1),
    )
    def pull_company_financial_data(symbols, output_dir):
        """
        TODO DOC
        """
        api_key = Variable.get("FMP_API_KEY", default_var="")
        outputs = scrape_financials_fmp.pull_fmp_financial_statements(symbols, output_dir, api_key)
        return outputs

    @task(
        retries=2,
        execution_timeout=datetime.timedelta(minutes=1),
        retry_delay=datetime.timedelta(minutes=1),
    )
    def retrieve_company_symbols(output_dir, **ctx):
        csv_path = ctx['params']['companySymbolCsv']
        symbol = ctx['params']['symbolColumn']
        s3_hook = S3Hook()
        s3_hook.download_file(
            key=csv_path,
            bucket_name=s3_bucket,
            preserve_file_name=True,
            use_autogenerated_subdir=False,
            local_path=output_dir
        )
        symbol_df = pd.read_csv(f"{output_dir}/{csv_path}")
        symbols = symbol_df[symbol].unique().tolist()
        symbol_slices = [x.tolist() for x in np.array_split(symbols, int(len(symbols)/10))]
        return list(symbol_slices)

    @task(
        retries=2,
        execution_timeout=datetime.timedelta(minutes=3),
        retry_delay=datetime.timedelta(minutes=1),
    )
    def upload_fmp_csv_s3(local_file_paths, s3_bucket, prefix):
        """
        Upload output CSV to S3 bucket
        """
        s3_hook = S3Hook()
        for fp in local_file_paths:
            file_name = fp.split('/')[-1]
            s3_hook.load_file(fp, f"company_financials_{prefix}/{file_name}", s3_bucket, replace=True)

    @task(
        retries=1,
        execution_timeout=datetime.timedelta(minutes=5),
        retry_delay=datetime.timedelta(minutes=1),
    )
    def merge_all_fmp_csv(local_file_paths, output_dir, s3_bucket, prefix):
        flatten_local_paths = list([p for sublist in local_file_paths for p in sublist])
        combined_df = pd.read_csv(flatten_local_paths[0])
        for fp in flatten_local_paths[1:]:
            df = pd.read_csv(fp)
            combined_df = pd.concat([combined_df, df], ignore_index=True)
        file_name = "combined_company_financials.csv"
        out_file = f"{output_dir}/{file_name}"
        combined_df.to_csv(out_file, index=False)
        s3_hook = S3Hook()
        s3_hook.load_file(out_file, f"company_financials_{prefix}/{file_name}", s3_bucket, replace=True)


    create_tmp_dir = BashOperator(
        task_id="create_tmp_dir",
        bash_command="mktemp -d 2>/dev/null"
    )
    company_symbol_list = retrieve_company_symbols(output_dir=create_tmp_dir.output)
    company_financial_csv_paths = pull_company_financial_data\
        .partial(output_dir=create_tmp_dir.output)\
        .expand(symbols=company_symbol_list)
    execute_time = datetime.datetime.now().strftime("%Y%m%d")
    upload_res = upload_fmp_csv_s3\
        .partial(s3_bucket=s3_bucket, prefix=execute_time)\
        .expand(local_file_paths=company_financial_csv_paths)
    merge_res = merge_all_fmp_csv(
        s3_bucket=s3_bucket,
        prefix=execute_time,
        output_dir=create_tmp_dir.output,
        local_file_paths=company_financial_csv_paths)
    remove_tmp_dir = BashOperator(
        task_id="remove_tmp_dir",
        bash_command="rm -rf {{ ti.xcom_pull(task_ids='create_tmp_dir') }}"
    )
    (upload_res, merge_res) >> remove_tmp_dir

scrape_company_financials()
